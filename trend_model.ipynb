{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ceaf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hlw world\n"
     ]
    }
   ],
   "source": [
    "print('hlw world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac37c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hlw\n"
     ]
    }
   ],
   "source": [
    "print('hlw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ae9552-e156-42f0-abea-3414701cf3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from bnltk.stemmer import BanglaStemmer\n",
    "from bnltk.tokenize import Tokenizers\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6648b919-6f92-4248-94b4-9d7585ad635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bangla_stopwords = [\n",
    "    'অতএব', 'অথচ', 'অথবা', 'অনুযায়ী', 'অনেক', 'অনেকে', 'অন্তত', 'অন্য', 'অবধি', 'অবশ্য', 'অর্থাৎ',\n",
    "    'আই', 'আছে', 'আজ', 'আপনার', 'আপনি', 'আবার', 'আমরা', 'আমাকে', 'আমাদের', 'আমার', 'আমি',\n",
    "    'আর', 'আরও', 'ই', 'ইত্যাদি', 'এই', 'এ', 'এঁদের', 'এঁরা', 'এটি', 'এটা', 'এটা', 'এদিকে', 'এবং', 'এবার',\n",
    "    'এমন', 'এমনকী', 'এরা', 'এল', 'এস', 'এসে', 'ঐ', 'ও', 'ওই', 'ওদের', 'ওর', 'কখনও', 'কত', 'কবে',\n",
    "    'কমনে', 'কয়েক', 'করবে', 'করবেন', 'করল', 'করলেন', 'করা', 'করানো', 'করায়', 'করার', 'করি', 'করেছে',\n",
    "    'করেন', 'করে', 'করেই', 'করেনি', 'কিছু', 'কিছুই', 'কিন্তু', 'কী', 'কেউ', 'কে', 'কেমন', 'কোথা', 'কোন',\n",
    "    'কোনও', 'কি', 'কিভাবে', 'কিছুক্ষণ', 'কেন', 'কেউই', 'গিয়ে', 'গিয়েছে', 'গেল', 'গেলে', 'চলে', 'ছাড়া',\n",
    "    'ছিল', 'ছিলাম', 'ছিলেন', 'ছিলো', 'জানেন', 'জন্য', 'জানে', 'জানানো', 'জানায়', 'জানিয়ে', 'টা',\n",
    "    'টি', 'তখন', 'তত', 'তবে', 'তুমি', 'তুলে', 'তেমন', 'তো', 'তোমার', 'তাদের', 'তাহলে', 'তা', 'তাঁদের',\n",
    "    'তাঁরা', 'তাঁর', 'তাঁরা', 'তাঁকে', 'তাঁরাও', 'তাদেরকে', 'তারা', 'তার', 'তারা', 'তাকে', 'তাহা', 'তিন',\n",
    "    'তিনি', 'তিনিও', 'তুমি', 'তোমরা', 'থাকবে', 'থাকবেন', 'থাকা', 'থাকায়', 'থাকে', 'থেকেও', 'থেকে',\n",
    "    'দিকে', 'দিয়ে', 'দিলেন', 'দিতে', 'দিয়ে', 'দুজন', 'দুটি', 'দুটো', 'দেওয়া', 'দেওয়া হয়', 'দেন', 'দেননি',\n",
    "    'ধরে', 'ধরে', 'নই', 'নয়', 'না', 'নিতে', 'নিজে', 'নিজেই', 'নিজের', 'নিজেদের', 'নেওয়া', 'নেওয়া হয়নি',\n",
    "    'নেওয়ার', 'নিয়েই', 'নিয়েছে', 'নিয়েছেন', 'নিয়ে', 'নেই', 'পক্ষে', 'পর', 'পরেও', 'পর্যন্ত', 'পরে', 'পাওয়া',\n",
    "    'পারে', 'পারেন', 'পারি', 'পি', 'পেয়েছি', 'প্রতি', 'প্রভৃতি', 'ফিরে', 'বদলে', 'বরং', 'বললেন', 'বললেন না',\n",
    "    'বললাম', 'বলতে', 'বলল', 'বললেন', 'বলছি', 'বলেছেন', 'বলে', 'বলেন', 'বলা', 'বলে', 'বসে', 'বা', 'বিনা',\n",
    "    'বিশেষ', 'ব্যবহার', 'ব্যবহার করে', 'ভাল', 'ভাবেই', 'মতো', 'মধ্যেও', 'মধ্যেই', 'মাধ্যমে', 'মোট', 'যখন',\n",
    "    'যত', 'যথেষ্ট', 'যদি', 'যাবে', 'যাওয়া', 'যাচ্ছে', 'যাতে', 'যার', 'যারা', 'যিনি', 'যে', 'যেকোনো', 'যেখানে',\n",
    "    'যেমন', 'যেতে', 'যেন', 'যেহেতু', 'যেয়ে', 'র', 'রকম', 'রেখে', 'রয়েছে', 'শুধু', 'সঙ্গে', 'সব', 'সবার',\n",
    "    'সবাই', 'সমস্ত', 'সম্পর্কে', 'সহ', 'সবারই', 'সামনে', 'সি', 'সে', 'সেই', 'সেখান', 'সেখানে', 'স্পষ্ট',\n",
    "    'হইতে', 'হইবে', 'হইয়াছে', 'হইয়েছে', 'হওয়া', 'হওয়াতে', 'হওয়ার', 'হওয়ায়', 'হচ্ছে', 'হত', 'হতে',\n",
    "    'হবেন', 'হয়ে', 'হয়', 'হয়তো', 'হয়নি', 'হয়েছে', 'হয়েছে', 'হল', 'হলে', 'হলো', 'হিসেবে'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af2bff1d-87ca-4492-892b-45e43c75adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizers()\n",
    "stemmer = BanglaStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a74b30-1ed7-4268-83bd-a7aa3c6fc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regex_tokenizer = RegexpTokenizer(r'[\\u0980-\\u09FF]+')\n",
    "def preprocess_bangla_text(text: str, stem: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    Comprehensive Bangla text preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # 1. Remove non-Bangla characters, URLs, numbers and punctuations\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF\\s]|[\\u09E6-\\u09EF]', '', text)\n",
    "    \n",
    "    # 2. Tokenize using regex tokenizer\n",
    "    tokens = regex_tokenizer.tokenize(text)\n",
    "\n",
    "    # 3. Stopword removal and aggressive stemming\n",
    "    processed = []\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "        if token not in bangla_stopwords:\n",
    "            if stem:\n",
    "                stemmed = stemmer.stem(token)\n",
    "                if stemmed:\n",
    "                    processed.append(stemmed)\n",
    "            else:\n",
    "                processed.append(token)\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6d43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "def detect_phrases(docs: list, min_freq: int = 10, \n",
    "                   bigram_threshold: float = 7.0, \n",
    "                   trigram_threshold: float = 12.0) -> dict:\n",
    "    \"\"\"\n",
    "    Detect meaningful phrases using PMI thresholds.\n",
    "    Returns: phrase mapping dictionary (e.g., {'রাজশাহীর আম': 'রাজশাহীর_আম'})\n",
    "    \"\"\"\n",
    "    # Flatten the list of tokenized documents into a single token stream\n",
    "    all_tokens = [token for doc in docs for token in doc]\n",
    "\n",
    "    # 1. Bigram detection using PMI\n",
    "    bigram_finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "    bigram_finder.apply_freq_filter(min_freq)\n",
    "    bigram_scored = bigram_finder.score_ngrams(BigramAssocMeasures.pmi)\n",
    "    significant_bigrams = [bigram for bigram, score in bigram_scored if score >= bigram_threshold]\n",
    "\n",
    "    # 2. Trigram detection using PMI\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(all_tokens)\n",
    "    trigram_finder.apply_freq_filter(min_freq)\n",
    "    trigram_scored = trigram_finder.score_ngrams(TrigramAssocMeasures.pmi)\n",
    "    significant_trigrams = [trigram for trigram, score in trigram_scored if score >= trigram_threshold]\n",
    "\n",
    "    # 3. Create a phrase mapping dictionary\n",
    "    phrase_map = {}\n",
    "    for bigram in significant_bigrams:\n",
    "        phrase = ' '.join(bigram)\n",
    "        merged = '_'.join(bigram)\n",
    "        phrase_map[phrase] = merged\n",
    "\n",
    "    for trigram in significant_trigrams:\n",
    "        phrase = ' '.join(trigram)\n",
    "        merged = '_'.join(trigram)\n",
    "        phrase_map[phrase] = merged\n",
    "\n",
    "    return phrase_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a11c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_phrases(docs: list, phrase_map: dict) -> list:\n",
    "    \"\"\"\n",
    "    Replace detected phrases in documents with merged tokens\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    for doc in docs:\n",
    "        text = ' '.join(doc)\n",
    "        # Replace phrases in descending length order (longest first)\n",
    "        for phrase, merged in sorted(phrase_map.items(), \n",
    "                                    key=lambda x: len(x[0]), \n",
    "                                    reverse=True):\n",
    "            if phrase in text:\n",
    "                text = text.replace(phrase, merged)\n",
    "        processed_docs.append(text.split())\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1102ea4-3399-452d-864c-cf33f24a7b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents\n"
     ]
    }
   ],
   "source": [
    "def load_documents(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    # Split documents by backslash (add strip() to remove whitespace)\n",
    "    documents = [doc.strip() for doc in content.split('\\\\') if doc.strip()]\n",
    "    return documents\n",
    "\n",
    "# Usage:\n",
    "bangla_documents = load_documents('pahelabaishak.txt')\n",
    "print(f\"Loaded {len(bangla_documents)} documents\")  # remove blank lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7470c8f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected Phrases:\n",
      "মি ফুট => মি_ফুট\n",
      "রবীন্দ্রনাথ ঠাকুর => রবীন্দ্রনাথ_ঠাকুর\n",
      "বিংশ শতাব্দীর => বিংশ_শতাব্দীর\n",
      "পঞ্জিকা অনুসার => পঞ্জিকা_অনুসার\n",
      "সত্যজিৎ রায় => সত্যজিৎ_রায়\n",
      "সম্রাট আকবর => সম্রাট_আকবর\n",
      "মঙ্গল শোভাযাত্ => মঙ্গল_শোভাযাত্\n",
      "গড় তাপমাত্ => গড়_তাপমাত্\n",
      "বেশ কয়েক => বেশ_কয়েক\n",
      "দেখা যায় => দেখা_যায়\n",
      "বিশেষভাব উল্লেখযোগ্য => বিশেষভাব_উল্লেখযোগ্য\n",
      "হিসাব ব => হিসাব_ব\n",
      "এর দশক => এর_দশক\n",
      "অফ ইন্ডিয়া => অফ_ইন্ডিয়া\n",
      "ইনস্টিটিউট অফ => ইনস্টিটিউট_অফ\n",
      "পয়লা বৈশাখ => পয়লা_বৈশাখ\n",
      "পহেলা বৈশাখ => পহেলা_বৈশাখ\n",
      "সাংস্কৃতিক ঐতিহ্য => সাংস্কৃতিক_ঐতিহ্য\n",
      "পূর্বতন নাম => পূর্বতন_নাম\n",
      "বিধানসভা কেন্দ্র => বিধানসভা_কেন্দ্র\n",
      "লোকসভা কেন্দ্র => লোকসভা_কেন্দ্র\n",
      "নাম পরিচিত => নাম_পরিচিত\n",
      "উৎসবগুলির মধ্য => উৎসবগুলির_মধ্য\n",
      "এগুলির মধ্য => এগুলির_মধ্য\n",
      "বৈশাখ উদ্যাপন => বৈশাখ_উদ্যাপন\n",
      "হ শতাংশ => হ_শতাংশ\n",
      "সাল হিসেব => সাল_হিসেব\n",
      "বাংলা দিনপঞ্জির => বাংলা_দিনপঞ্জির\n",
      "কলকাতায় অবস্থিত => কলকাতায়_অবস্থিত\n",
      "নতুন বছর => নতুন_বছর\n",
      "কলকাতা পৌরসংস্থ => কলকাতা_পৌরসংস্থ\n",
      "বৃহত্তর কলকাত => বৃহত্তর_কলকাত\n",
      "বাংলা সন => বাংলা_সন\n",
      "কলকাতা পৌরসংস্থা => কলকাতা_পৌরসংস্থা\n",
      "হিসেব কলকাতা => হিসেব_কলকাতা\n",
      "প্রথম দিন => প্রথম_দিন\n",
      "পূর্ব ভারত => পূর্ব_ভারত\n",
      "মধ্য উল্লেখযোগ্য => মধ্য_উল্লেখযোগ্য\n",
      "বাংলা নববর্ষ => বাংলা_নববর্ষ\n",
      "মাস দিন => মাস_দিন\n",
      "কলকাতা উত্তর => কলকাতা_উত্তর\n",
      "এর মধ্য => এর_মধ্য\n"
     ]
    }
   ],
   "source": [
    "# Example Usage Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample Bangla dataset (replace with your actual data)\n",
    "    # bangla_documents = [\n",
    "    #     \"রাজশাহীর আম বাংলাদেশের সেরা আম হিসেবে পরিচিত\",\n",
    "    #     \"চাঁপাইনবাবগঞ্জে আমের বাম্পার ফলন হয়েছে এবার\",\n",
    "    #     \"আম রাজশাহী ও চাঁপাইনবাবগঞ্জ অঞ্চলের অর্থনীতির মূল চালিকাশক্তি\",\n",
    "    #     \"আম রপ্তানিতে নতুন রেকর্ড গড়ল বাংলাদেশ\"\n",
    "    # ]\n",
    "    # bangla_documents = read_bangla_text_file(\"pahelabaishak.txt\")\n",
    "    \n",
    "    \n",
    "    # 1. Preprocess all documents\n",
    "    preprocessed_docs = [preprocess_bangla_text(doc) for doc in bangla_documents]\n",
    "    \n",
    "    # print(\"Preprocessed Tokens:\")\n",
    "    # for i, doc in enumerate(preprocessed_docs):\n",
    "    #     print(f\"Doc {i+1}: {doc}\")\n",
    "    \n",
    "    # 2. Detect significant phrases using PMI\n",
    "    phrase_mapping = detect_phrases(preprocessed_docs, \n",
    "                                  min_freq=5, \n",
    "                                  bigram_threshold=4.0,\n",
    "                                  trigram_threshold=8)\n",
    "    \n",
    "    print(\"\\nDetected Phrases:\")\n",
    "    for phrase, merged in phrase_mapping.items():\n",
    "        print(f\"{phrase} => {merged}\")\n",
    "    \n",
    "    # 3. Integrate phrases into documents\n",
    "    final_docs = integrate_phrases(preprocessed_docs, phrase_mapping)\n",
    "    \n",
    "    # print(\"\\nFinal Processed Documents with Phrases:\")\n",
    "    # for i, doc in enumerate(final_docs):\n",
    "    #     print(f\"Doc {i+1}: {doc}\")\n",
    "\n",
    "# Output will show processing stages:\n",
    "# Preprocessed Tokens: Stemmed tokens without stopwords\n",
    "# Detected Phrases: Found significant n-grams\n",
    "# Final Documents: With phrases merged (e.g., [\"রাজশাহীর_আম\", \"বাংলাদেশের\", ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f8ef4-4442-4cfb-8877-a1aa65984700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dictionary stats:\n",
      "Number of tokens: 3128\n",
      "Sample tokens: [(0, 'অংশ'), (1, 'অংশগ্রহণ'), (2, 'অংশগ্রহণকারী'), (3, 'অগ্রগতির'), (4, 'অগ্রহায়ণ')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "# 1. Prepare Corpus (using your preprocessed docs)\n",
    "# -------------------------------------------------\n",
    "# Assuming you already have:\n",
    "# final_docs = [['পহেলা', 'বৈশাখ', 'বাংলা'], ['নববর্ষ', 'উৎসব'], ...] \n",
    "\n",
    "dictionary = Dictionary(final_docs)\n",
    "\n",
    "print(\"\\nDictionary stats:\")\n",
    "print(f\"Number of tokens: {len(dictionary)}\")\n",
    "print(\"Sample tokens:\", list(dictionary.items())[:5])\n",
    "\n",
    "dictionary.filter_extremes(no_below=1, no_above=1.0)  # Adjust thresholds\n",
    "corpus = [dictionary.doc2bow(doc) for doc in final_docs]\n",
    "\n",
    "# 2. Train LDA Model\n",
    "# -------------------\n",
    "NUM_TOPICS = 2 # Reduced from 5 for more focused topics\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    random_state=42,\n",
    "    passes=50,  # Increased iterations\n",
    "    alpha='symmetric'  # Favors sparse topic distributions\n",
    ")\n",
    "\n",
    "# 3. Evaluate Model Quality\n",
    "# --------------------------\n",
    "coherence_model = CoherenceModel(\n",
    "    model=lda_model,\n",
    "    texts=final_docs,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "print(f\"Coherence Score: {coherence_model.get_coherence():.3f}\")\n",
    "\n",
    "\n",
    "# 5. Query Processing Function\n",
    "# ----------------------------\n",
    "def get_related_topics_enhanced(user_query, threshold=0.1):\n",
    "    query_tokens = preprocess_bangla_text(user_query)\n",
    "    query_bow = dictionary.doc2bow(query_tokens)\n",
    "    \n",
    "    # Get document-topic distribution\n",
    "    topic_dist = lda_model[query_bow]\n",
    "    \n",
    "    # Filter topics above threshold\n",
    "    relevant_topics = [(topic_id, prob) \n",
    "                      for topic_id, prob in topic_dist \n",
    "                      if prob >= threshold]\n",
    "    \n",
    "    # Sort by probability\n",
    "    relevant_topics.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for topic_id, prob in relevant_topics:\n",
    "        top_words = [word for word, _ in lda_model.show_topic(topic_id, topn=7)]\n",
    "        results.append({\n",
    "            'topic_id': topic_id,\n",
    "            'probability': f\"{prob:.2%}\",\n",
    "            'top_words': \", \".join(top_words[:5]),\n",
    "            'all_keywords': top_words\n",
    "        })\n",
    "    return results\n",
    "def display_results(user_input):\n",
    "    topics = get_related_topics_enhanced(user_input)\n",
    "    \n",
    "    print(f\"\\nQuery: '{user_input}'\")\n",
    "    print(f\"Coherence Score: {coherence_model.get_coherence():.3f}\")\n",
    "    print(\"\\nMost Relevant Topics:\")\n",
    "    \n",
    "    for i, topic in enumerate(topics, 1):\n",
    "        print(f\"{i}. Topic {topic['topic_id']} (Relevance: {topic['probability']})\")\n",
    "        print(f\"   Keywords: {topic['top_words']}...\")\n",
    "        if i == 1:  # Show full keywords for top topic\n",
    "            print(f\"   All Keywords: {', '.join(topic['all_keywords'])}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# 6. Example Usage\n",
    "# -----------------\n",
    "display_results(\"বাংলাদেশ\")\n",
    "display_results(\"পহেলা বৈশাখ\")\n",
    "display_results(\"মঙ্গল শোভাযাত্রা\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48901c-6611-448c-8b21-eb600c1fbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(\"কলকাতা\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbf23c-c27b-4f88-a4ab-6d17e609477d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_lda_topics(model, num_words=10):\n",
    "    print(\"\\nDiscovered Topics:\")\n",
    "    print(\"-----------------\")\n",
    "    for topic_id in range(model.num_topics):\n",
    "        words = model.show_topic(topic_id, topn=num_words)\n",
    "        topic_words = \", \".join([word for word, prob in words])\n",
    "        print(f\"Topic {topic_id}: {topic_words}\")\n",
    "    \n",
    "    # Print most representative document per topic\n",
    "    print(\"\\nMost Representative Documents per Topic:\")\n",
    "    print(\"---------------------------------------\")\n",
    "    for topic_id in range(model.num_topics):\n",
    "        print(f\"\\nTopic {topic_id} Representative Text:\")\n",
    "        # Get document most focused on this topic\n",
    "        doc_topics = [model.get_document_topics(doc) for doc in corpus]\n",
    "        doc_id = np.argmax([dict(topics).get(topic_id, 0) for topics in doc_topics])\n",
    "        print(\" \".join(final_docs[doc_id][:100]) + \"...\")  # First 100 words\n",
    "\n",
    "# Usage\n",
    "print_lda_topics(lda_model, num_words=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfad0a-ef67-47f5-80c6-20555b2cdbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9a4e5-6149-4883-b1ed-1d9fe8963eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "# Prepare visualization\n",
    "vis_data = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Display (in notebook) or save to HTML\n",
    "pyLDAvis.display(vis_data)  # For Jupyter notebook\n",
    "# pyLDAvis.save_html(vis_data, 'bangla_topics.html')  # To save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501d6c0-7f08-438d-a2d3-ea4e5f05c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(\"শহর\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263db5ca-6624-4fb9-9062-650f66be4056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Gensim)",
   "language": "python",
   "name": "gensim-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
